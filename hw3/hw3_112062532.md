---
title: HW3 Parallel Programming

---

# HW3 Parallel Programming
**姓名:** 温佩旻   **學號:** 112062532
## 1. Implementation 
### a. Which algorithm do you choose in hw3-1?
這邊是直接使用Floyd-Warshall algorithm的三層for迴圈來實作，並使用OpenMP在處理initial data及計算時做平行，schedule是設定為static。

![image](https://hackmd.io/_uploads/HJmBdRp7Jl.png)

### b. How do you divide your data in hw3-2, hw3-3?
這邊就是照著SPEC上面的演算法去刻出來，總共跑 ```n/blocking factor``` 次的phase1、2、3，為了避免在phase的計算中因為 ```n/blocking factor``` 不能整除而導致額外的if-else判，我在初始化資料時會先將data進行**padding**，如果n不能整除blocking factor，會將n補齊到可以整除的數字，除了可以簡化條件判斷，還能有效減少 **bank conflict**。
在hw3-3中，因為需要在兩個GPU上分工處理，會需要分配gpu負責處理的data範圍，這邊我是將Dist矩陣分成上下兩部來做處理。
  
### c. What’s your configuration in hw3-2, hw3-3? And why? (e.g. blocking factor, #blocks, #threads)
設置的部分我hw3-2和3-3都是一樣的，下面就一起寫。
1. **blocking factor設為64**，主要是基於phase 1、2、3所需要的shared memory大小來設置，在phase 2、3中，每個phase都需要分配3個大小為```blocking factor * blocking factor```的shared memory，考慮到shared memory最大容量為49152 byte，每個元素占用 4 bytes，因此可用元素總數為 49152 / 4 = 12288，再除以所需的 3 個區塊，可以得出blocking factor 最大為 64，能充分利用shared memory空間。
2. **#threads = (32 * 32)**，這邊是考慮到在一個gpu中每個block最多可以有1024個thread，這樣設置可以最大化利用GPU thread的資源，並且符合一個 warp（32 個 threads）執行的機制。
3. **#blocks**
    * phase 1:  **#blocks = 1**
        只需要對pivot做更新，所以只需要一個block
    * phase 2:  **#blocks = (n/blocking factor-1, 1)**
        只需要對col、row做更新，所以只需要 ```n/blocking factor-1``` 個block，每個thread會負責計算col中的一個值和row中的一個值，這邊會減1是因為pivot那一格不用更新，可以扣除，這樣可以少算一個block。
    * phase 3:  **#blocks = (n/blocking factor-1, n/blocking factor-1)**
        這邊需要對除了pivot、pivot的col、pivot的row以外的block做更新，所以需要```n/blocking factor-1 * n/blocking factor-1```個block，這邊減1也是為了少算一個col和一個row，可以減少計算時間。
4. **#round = padding後的n / blocking factor** ，這邊就是根據SPEC中提供的演算法去設定的。

### d. How do you implement the communication in hw3-3?
因為phase 3佔總計算時間90%以上，這邊只有在phase 3的時候有分兩張GPU算不同的data，在phase 1和phase 2兩張GPU都是全部一起算，避免再phase 1、2浪費時間在做memory copy。
在每個round要運算前，如果這個round範圍在我負責的row範圍中，就把這個round的pivot block data透過cudaMemcpyDeviceToDevice傳給另一個gpu。

### e. Briefly describe your implementations in diagrams, figures or sentences.
下圖是模擬我在每個block中如何將32 * 32的thread分配處理64 * 64的資料，每個thread會需要處理四筆資料上下左右各差32(紅色圈圈的部分是同一個thread負責)，這樣才能將讓shared memory最大化被利用，同時因為一個warp是32個thread，這樣的分配也能確保memory coalescing。
![image](https://hackmd.io/_uploads/BJrXwgAm1l.png)
在phase 1 中，只需要更新pivot的資料，所以只會處理一個64 * 64大小的資料量，那就如上圖那樣分配去計算。
在phase 2 中，需要利用pivot去更新col、row的結果，因此每個block除了要將pivot資料載入shared memory中，也需要各自負責一個row和一個col block載入shared memory的問題。
在phase 3 中，每個block則是需要利用row和col的資料去更新剩餘資料，因此每個block需要負責載入一個待更新的block(紅色部分)，一個row block，及一個col block。

![image](https://hackmd.io/_uploads/SJw6dxA71e.png)


## 2. Profiling Results
這裡我使用p11k1這筆測資來測loading最重的phase 3
| Metric name | Min | Max | Avg |
| -------- | -------- | -------- |----------|
| sm_efficiency  | 99.80%  | 99.92%  | 99.89% |
|shared_load_throughput | 3385.9 GB/s  | 3448.6 GB/s |3428.8 GB/s|
|shared_store_throughput | 283.71 GB/s  |287.16 GB/s | 285.3 GB/s|
|gld_throughput | 141.08GB/s |143.69GB/s | 142.87GB/s|
|gst_throughput | 70.927 GB/s |71.86 GB/s | 71.336 GB/s|
|achieved_occupancy|0.923959|0.928615|0.925922|

可以看到SM 使用率 (sm_efficiency) 幾乎 100%，且 Achieved Occupancy 也有9成左右，代表GPU thread 執行效率也不錯。shared memory和global memory的 throughput 也符合我程式撰寫的邏輯，大量的load shared memory來減少global memory access的次數。

## 3. Experiment & Analysis
### a. System Spec
使用課堂上給的GPU Apollo Server
### b. Blocking Factor
這邊我是使用c21.1這筆測資來profile，因為再往後的側資都會profile timeout。一樣觀察loading最重的kernel 3。 
|![image](https://hackmd.io/_uploads/HJ78VXkByx.png)| 
|:------------------:|
|圖1|

圖1我是使用nvprof的```inst_integer```這個metric去進行profile，得到phase 3的integer instruction count，再拿去除phase 3的執行時間的到的結果。可以發現blocking factor和 Integer GOPS 的關係並非完全正相關，這邊我猜測是因為我設定```threadPerBlock```時是根據blocking factor去設定的，假設blocking = 8，那```threadPerBlock```就是設定為(8, 8)，因為每個block裡面最多開1024個thread，所以這邊最多就是開到(32, 32)，那這邊可以看到在blcoking factor = 32的時候，GOPS有稍微下降，我認為是因為剛好產生出來的 pipeline、記憶體存取、occupancy 等因素無法達到理想平衡，所以才導致這樣的結果，但大致上來說是有隨著Blocking factor上升，GOPS跟著上升的。
圖2、3分別是使用nvprof的```gst_throughput、gld_throughput、shared_load_throughput、shared_store_throughput```這4個metric去進行profile。
* global mamory bandwidth
    可以看到圖2顯示blocking factor比較低的兩個值(8、16)反而有比較高的throughput，這可能是因為較小的區塊大小(blocking factor較小)導致在該計算階段中，需要頻繁、連續的從 global memory load資料，導致記憶體流量高。而在較高的blocking factor(32, 64)時，global memory 的總 throughput 明顯下滑，這可能是因為程式在較大 blocking factor 下執行邏輯改變，可能會比較依賴shared memory，而不是global memory。

* shared mamory bandwidth
     這邊因為shared store throughput起伏不大，就專注在討論shared load throughput。可以在圖3看到和圖2相反的情況，隨著blocking factor越大，bandwidth會持續往上攀升，主要原因應該是當blocking factor增大，程式可能在每個kernel執行階段中對同一組資料進行更多次的重複計算，但透過shared memory暫存，可減少多次從 global memory 載入，進而加強 shared memory 的利用率，因此load shared memory操作會更頻繁且更密集，導致throughput 上升。
     
|![image](https://hackmd.io/_uploads/H1eFzQySJx.png)|![image](https://hackmd.io/_uploads/SJb9Mm1Syl.png)|
|:------------------:|:------------------:|
|圖2|圖3|

### c. Optimization 
這邊使用```p28k1```這筆測資去做實驗。
圖4中的Basic GPU版本是將```blocking factor```設為32，```threadPerBlock```也設為32，沒有使用到任何shared memory，也沒有做任何padding，單純只把phase1、2、3改成kernel，access golbal memory
![image](https://hackmd.io/_uploads/B1uiJmJryx.png)

1. padding
    因為n有可能不是blocking factor的整數被，這樣會導致在每個kernel中都需要特別判斷有沒有超出範圍，會有很多if-else的判斷，所以這邊將n padding 到能和```blocking factor```整除的最小數字，減少kernel中的branch，不過從圖中來看其實並沒有差太多。
2. shared memory
    這邊將global memory 重複被使用的資料 load 進 shared memory 去存取，減少重複 access global memory的次數。 實作的方式是讓每個thread對應到一筆data取更新，可以發現performance的確有了非常顯著的提升。
3. Large blocking factor
    這邊嘗試將```blocking factor```往上調，讓一個thread負責處理四筆資料，存取方式如上面Implement-e提到的一樣，因為每個warp是32個一組，這樣分配可以確定每個warp每次同時存取到的部分都是連續的，從實驗結果來看也符合我的預期，Performance可以再往上提升。
4. unroll
    這邊希望透過unroll來提升Performance，但從結果來看似乎沒有什麼幫助，甚至可能還比沒有unroll的效果來的差，這邊我猜測應該是因為compiler已經對迴圈做優化了，所以unroll就沒什麼效益了。
5. CUDA-Based initial + pinned memory 
    這邊將initial dist的部分改成用cuda去實作，並且將device端用來存結果的Dist pinned 住，發現對於Performance也有些微幫助。
    
6. Enhancing GPU Data Output Efficiency
    因為將 device 資料複製回 host 資料，再透過fwrite去寫入結果的效能不是太好，這邊嘗試將 output file 用 mmap 對應到一塊可讀寫的記憶體區域 `mapPtr`，再將 `mapPtr` 用 `cudaHostRegister(mapPtr, outSize, cudaHostRegisterDefault` 的方式 pin 住，直接將 device 中的結果用 `cudaMemcpy2D` 從 device copy 到 `mapPtr`，就不需要再做額外寫檔的操作，實作下來的結果也確實有些微改善 performance，而且能多過一筆測資。

7. Replace Phase 3 shared memory with registers
    這邊是將 phase 3 中的其中一塊 thread 間沒有重複讀寫的 shared memory (也就是每個thread 要負責更新的那四個位置) 改成用register來存，可以發現幫助不是特別大，大概就差個0點幾秒，不過可以更穩定的過測資。

### d. Weak scalability
由於weak scalability是希望在增加硬體資源時，隨著工作量成比例增加，能保持穩定的效能。而Floyd Warshall演算法的時間複雜度O($n^3$)，因此這邊挑選兩筆測資的vertex數量(n)需要能滿足: $2(n_1)^3$ $\approx$ $(n_2)^3$，最後選擇的測資是 **p23k1**(n = 22973) 及 **p29k1**(n = 28911)。
| Test Case | #of GPUs | Total Time(s) | Computation Time(s) |
| -------- | -------- | -------- |----------|
| p23k1  | 1  | 15.2904 | 11.1546 |
| p29k1  | 2  | 24.3789 | 14.9273 |

從上面的圖表中在phase 1、phase 2沒有優化的情況下，可以看到在總執行時間上weak scalability不是太好，原因應該是在多 GPU 下的communication或同步等其他開銷導致了時間增加，但在coputation time上就相對穩定，代表kernel計算部分分配給多GPU的方法是合理的。

### e. Time Distribution
這邊我使用NVTX來算出下列三個時間分布：
* **I/O:** 在input和output的前後都加上nvtx push/pop。
* **computation:** 在Phase 1、2、3執行的總時間。
* **memory copy:** 在cudaMemcpy的前後加上nvtx push/pop。

測資使用p15k1、p20k1、p25k1、p30k1，畫出下圖。
![image](https://hackmd.io/_uploads/rJFq1ceEyx.png)

上圖可以看到vertex(n)數量和時間呈正相關，尤其是計算時間，所以在程式中主要都是對compute做優化，但即使我在上面提到很多有實作的優化，看似已經將Performance提升許多，但當要處理的data數量增加，在computation time 的 speedup 效果也很有限，除此之外也可以發現在memory copy的時間幾乎少到可以忽略。

## 4. Experiment on AMD GPU
我在將我的hw3-2從Nvidia轉至AMD時發現在AMD上跑的結果都是wrong answer，後來發現是因為我把phase1、2中的```__syncthreads()```拿掉導致的，當時只是想測試看看是不是可以拔掉不做sync，在NV上也沒有發生錯誤，當時也覺得很疑惑，因為理論上應該要sync才對，不過因為NV跑的結果對，就沒有再把他加回去了，直到在AMD上跑錯誤才想起，後來發現好像是因為Nvidia在同一個warp的執行中有內建同步特性，也就是說硬體本身在部分情況下容忍一些同步錯誤，就算你該同步的地方沒寫道也是會得到正確結果，但AMD要求較嚴格的同步，不會自動幫你同步，所以少寫```__syncthreads()```就會導致錯誤。

## 5. Experience & conclusion
這份作業讓我深刻體會到Cuda撰寫的困難點，除了應該要思考如何最大化利用shared memory，減少重複access global memory的次數之外，還需要思考如何分配#block、一個thread要怎麼負責多個data運算、global memory 如何 mapping 到 shared memory等等的問題，這是以前寫CPU程式不曾思考過的地方。