---
title: HW4 Parallel Programming

---

# HW4 Parallel Programming
**姓名:** 温佩旻   **學號:** 112062532
## 1. Implementation 
### a. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (ℓ and 𝑚) were calculated.
主要有 5 個 step 如下，總共會做 B * (N/B_r) * (N/B_c) 次。
1. `QKDotAndScalar`
    每一round的Q大小為 B_r × d、K 的大小為 B_c × d。
    matrix Q 和 matrix K 做 dot，再乘上 scalar 會得出 `Sij`，大小為 B_r * B_c ，其中 shared memory 是 2 * 32 * 32 大小(如圖1)，usage大概是8GB。
2. `RowMax`
    從步驟一得到大小為 B_r * B_c 的 Sij，接著需要計算每一個 row 的 當下這一個round 的 max值，並記錄在mij，大小為 B_r，在UpdateMiLiOi 時會需要用到，這邊因為沒有reuse 的 data，所以沒有使用shared memory。
3. `MinusMaxAndExp`
    從步驟二可以得到當下這一 round，每一個 row 的 max 值，此時再把步驟二得到的Sij，每個element 減掉該element 的 row max，再取exp，得到大小一樣為 B_r * B_c 的 Pij，這個步驟一樣沒有使用到 shared memory。
4. `RowSum`
    這個步驟類似於步驟二，需要將步驟三拿到的 Pij 把每一 row 加起來，並記錄在 lij，大小為 B_c，在UpdateMiLiOi 時會需要用到，這邊因為沒有reuse 的 data，所以沒有使用shared memory。
5. `UpdateMiLiOi`
    這個步驟需要透過步驟二及步驟四取得的 mij 和 lij 來去將 Oi 做更新，其中需要先將 mij 先去和目前 global的 row max(mi) 值做比較，得到mi_new，lij 也需要和 global 的 row sum (li) 值去做加總計算，得到li_new，接著需要將步驟三的 Pij 拿來和 Vj 做 pv dot，這邊的 Vj 一個 round 大小是 B_c * d，但因為 pv dot 會讓 v 是 column based 存取，如果在global memory 的話效率會很差，所以這邊會把 Vj 放進 shared memory(如圖2)，大小是 B_c * d，接著再做pv dot，如下圖，每個thread會負責d個oi_d的更新。
   ![image](https://hackmd.io/_uploads/H1klosDSye.png)

實驗過後發現將步驟二、三、四、五合併成一個kernel會有比較好的效果，所以最終這五個步驟會整合成兩個kernel，一個負責做QKDotAndScalar，另一個就是做其他所有步驟。


### b. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.
* matrix Q
    Q 是一個 N * d 矩陣，會和 K 做 QK dot，所以 Q 是 row based 的存取 ，因此將 Q 的 N 個 row 拆分成 B_r 個 row 為一組，每次 launch 一個 `QKDotAndScalar` 他的 Q 就是一個  `B_r * d` 大小的矩陣，假設 `BLOCK_SIZE = 32`，則一個 block 要處理 B_r/32 * d 大小的 Q，Q 沿著 d 的方向每次同時取出 32 個 element 做為一個 tile，透過 `threadIdx.x`、`threadIdx.y`，每 thread 對 Q 這個 tile 中的一個元素進行讀取並存進shared memory，這樣一個 block 就可以將這一塊 32×32 的 Q 資料載入到 q_shared 中再進行計算，使得多個 block 能夠同時平行處理 Q 的不同區塊。

* matrix K
    K 是一個 N * d 矩陣，會和 Q 做 QK dot，K 的原始儲存方式與 Q 相同（row-based），但為了計算 QK dot，所以在將 K 載入 shared memory 時，透過 `k_shared[threadIdx.x][threadIdx.y] = ... ` 的 indexing pattern，對 K 的一塊 32×32 block 做類似轉置的動作，所以 K 在global 實際上是 row based 的存取 ，因此將 K 的 N 個 row 拆分成 B_c 個 row 為一組，每次 launch 一個 `QKDotAndScalar` 他的 K 就是一個  `B_c * d` 大小的矩陣，假設 `BLOCK_SIZE = 32`，則一個 block 要處理 B_c/32 * d 大小的 Q，Q 沿著 d 的方向每次同時取出 32 個 element 做為一個 tile，透過 `threadIdx.x`、`threadIdx.y`，每 thread 對 K 這個 tile 中的一個元素進行讀取並存進shared memory，這樣一個 block 就可以將這一塊 32×32 的 K 資料載入到 k_shared 中再進行計算，使得多個 block 能夠同時平行處理 K 的不同區塊。
    
* matrix V
    V 是一個 N * d 矩陣，會和大小為 B_r * B_c 的 `Pij` 做 pv dot，所以 V 是 column based 的存取 ，每次 launch 一個 `UpdateMiLiOi`，他的 V 就是一個 `B_c * d` 大小的矩陣，因為在每個 block 中 要處理的 V 是一樣的，因此這邊的平行地方就變成是每個 block 中的 thread 去平行 load global 的 V 到 shared memory ，假設 `BLOCK_SIZE = 32`，因為考慮到記憶體連續存取的問題，所以會沿著 d 的方向每次同時取出 32 個 element 存進 shared memory，直到將 `B_c * d` 大小的 shared_vj_d store 完畢，才去做 pv 計算。
    
### c. Describe how you chose the block sizes B_r and B_c and why.
`B_r = N`、`B_c = 64`，一開始是設定 `B_r = 32、B_c = 32`，但無論怎麼優化code，Performance 都沒有顯著提升，直到將 `B_r` 從32開始倍數往上調才有非常明顯的改善，因為一個kernel處理的量變多了，也不會launch那麼多kernel，大幅減少整體時間。而`B_c` 設定成64會比原先的32效果好一些，原因和B_r的一樣，不過再往上調就沒有效果了，而且因為shared memory和B_c大小有關係，所以在往上調也很可能會直接導致shared memory使用量超過上限，因此最後`B_c`設定為64。

### d. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
我的程式有三種kernel，分別是`init_ml_kernel`、`QKDotAndScalar`、`UpdateMiLiOi`。
* `init_ml_kernel`
    用來初始化 m (每個 row 的 max 值)、l(每個 row 的 sum)，```threadsPerBlock = min(1024, N)```、因為每個 m 和 l 的大小是 N，所以 `blocksPerGrid = N / threadsPerBlock`，讓每個 thread 去處理一個 m、l 的初始化。這邊因為沒有重複使用的global memory，每個 data 都只會從global memory load 一次，所以沒有用 shared memory。

* `QKDotAndScalar`
這個 kernel 一次處理的 matrices  Q大小是 `br * d`，matrices  K大小是 `d * bc`，會做QK dot 再乘以 scalar 來算出 `Sij`，`Sij` 算出來大小會是 `br * bc`，所以這邊設定 `blockDimQK = (32, 32)`、`gridDim = (bc / blockDimQK.x, br / blockDimQK.y)`，讓每個 thread 負責 `Sij` 裡面一個值的計算。
shared memory的部分則是 QK 各開 `BLOCKSIZE * BLOCKSIZE` 個，這邊的 `BLOCKSIZE = 32`，也就是一個 thread 負責負責從global memory 中 load 一個 Q 及一個 K 裡面的一個值，再去算product 和 sum ，接著再讓每個 thread 負責 `Sij` 裡面一個值的計算。
    |![image](https://hackmd.io/_uploads/HkotEmWBkx.png)|
     |:-----------------:|
    |圖 1 QKDotAndScalar kernel|


* `UpdateMiLiOi`
這個 kernel 負責的是將上一個 kernel (`QKDotAndScalar`) 算出來大小為 `br * bc` 的 `Sij` 做 RowMax，再做 Minus max and exp and row max，最後再算 pv ，然後更新回Oi。
這邊考慮到要做 RowMax 和 RowSum，所以 `threadsPerBlock` 設定為 `BLOCKSIZE(32)`，一共有  br 個 row，所以 `blocksPerGrid = br / threadsPerBlock`。
shared memory 的部分這邊只有 matrices  V 有存成 shared memory，因為 matrices  V 原先在global 存取方式是 column based ，會很慢，所以要透過讓他在 global memory 被存取的時候是連續存取的，再將他放進 shared memory，這樣再做 p * v 的時候雖然 v 還是column based 的存取，可是因為再shared memory，所以速度會比較快，其中 `shared_vj_d` 大小為 `bc * MAX_D`，`MAX_D` 設定為64，因為 d 最大是 64。至於這邊沒有將 `Pij` 和 `Sij` 用成shared memory 的原因在之後的 Optimization 第5點有特別提到。
    |![image](https://hackmd.io/_uploads/HkTTgNZrJg.png)|
    |:-----------------:|
    |圖 2 將 matrices  V 從global memory load 進 shared memory|
 
### e. Justify your choices and how they relate to the blocking factors and the SRAM size.
這邊說明一下比較重要也較耗時間的兩個kernel。
在 `QKDotAndScalar` 中考慮到只需要做內積相乘，所以才將 `blockDim` 設定為 `(blocksize, blocksize)`，其中 `blocksize = 32`，讓 QK 在 mapping shared memory的時候是以 `blocksize * blocksize` 大小去做 shift，會比只設定 `threadsPerBlock = blocksize` 來的有效率，而這樣一來SRAM 也只需要用到 `2 * blocksize * blocksize * 4` bytes，不會讓 shared memory 的 loading 太重。
在 `UpdateMiLiOi` 中因為要處理的事情比較多，包括每個 row 找 max 及 sum，所以才將 `threadsPerBlock = blocksize(32)` 不是設定為二維的blockDim，而 `blocksize` 只設 32 的原因是在 mapping vj 的shared memory 的時候，為了讓 access global memory 的記憶體能連續，是沿著 d 方向每次從 global access `blocksize` 個到shared memory，如果  `blocksize` 調大，那可能會超出 d 的範圍(d是32或64)，導致mapping錯誤，所以在這邊只能將blocksize設定為32。SRAM size的部分則是被 bc bound 住，不過從我的code去實驗也發現shared memory不能用太多，反而會導致平行度下降(在Optimization第五點有比較詳細解說)，所以這邊SRAM 只有用到 `bc * d(64) * 4` bytes。

## 2. Profiling Results
這邊我是使用 `t10` 這筆測資來profile，因為再往後的側資都會profile timeout，觀察loading 最重的kernel `UpdateMiLiOi`。
| Metric name | Min | Max | Avg |
| -------- | -------- | -------- |----------|
| sm_efficiency  |75.97%|78.91%|77.93%|
|shared_load_throughput |177.56GB/s|184.44GB/s|181.96GB/s|
|shared_store_throughput |177.56GB/s|184.44GB/s|181.96GB/s|
|gld_throughput |38.884GB/s|40.392GB/s|39.848GB/s|
|gst_throughput | 11.141GB/s|11.573GB/s | 11.417GB/s|
|achieved_occupancy|0.041538|0.041941|0.041801|

整體來說 SM 的使用率（sm_efficiency) 算高，shared memory 的使用效率也很高，不過在achieved_occupancy 卻只有約 4%，我猜想可能是我的block/thread的操作沒有寫好，導致一次能執行的 thread 數量相對受限，也可能是 flash attention 對memory 資源的需求高，才導致這樣的結果。

## 3. Experiment & Analysis
### a. System Spec
使用課堂上給的`Apollo GPU` Server

### b. Optimization 

這邊使用 `t30` 這筆測資去做實驗，因為 `t25~t30` 的測資 `N` 都很大，而我主要做的優化是在優化 `N` 很大時候的效能，所以這邊的實驗就用 `t30` 來做。
圖中的 GPU_basline 是直接從sequential版本的改。一共分成5個kernel，分別為 `QKDotAndScalar`、`RowMax`、`MinusMaxAndExp`、`RowSum`、`UpdateMiLiOi`。其中 `threadsPerBlock = 32`、`B_r = 32`、`B_c = 32`，QKV 三個 matrices  會 load 到 shared memory 再去做 dot。

![image](https://hackmd.io/_uploads/ByoqJWZHJl.png)

1. Merged 5 kernels into 2 kernels
這邊的想法是想減少 kernel launch 的次數，因為從 `Nsight System` 發現 kernel launch時間佔了很多時間，所以將原先的5個kernel縮減成2個kernel(`QKDotAndScalar`、`UpdateMiLiOi`)，同時也可以減少 store 到 global memory 的次數，像是`mij`、`lij`這些中間產出的值可以直接用 register 存，實驗結果也確實有提升一些效能。
除此之外其實也有嘗試全部合併成一個kernel，但效能會變很差，再跑 `t30` 這筆測資的時候會直接跑不出來，因此這邊就沒有畫在圖中。

2. br = 64
這邊的想法也是想再繼續減少 kernel launch 的次數，因為在 `B`、`N` 很大的時候，如果 `br`、`bc` 都設定為32，那會 launch 非常多 kernel，以 `t30` 為例，`N = 32768`，那launch 的 kernel 數量就會是 $(32768/32) * (32768/32)$，launch 超過 $10^6$ 個，所以這邊把 br 調成 64，發現 Performance 有非常顯著的成長，會直接縮短一倍的時間。

3. br = 128
這邊延續上一個優化，再繼續把 `br` 往上調一倍，發現時間也能再縮短一倍的時間。

4. br = N
這邊繼續延續上兩個優化，因為發現把 br 依倍數調高，能讓 performance 時間有線性成長，所以直接將 `br` 設定成 N，可以從實驗結果發現花費時間直接縮短至 5 秒，有非常好的效果。

5. reduce shared memory pressure
從 `Nsight System` 發現我的 code 大多數時間花費在 `UpdateMiLiOi` 這個 kernel，再從編譯出來的資訊可以發現這個kernel的register用了37個，一個 SM 的 register 大小是 65536，以 register 來說，最大平行度是 $65536/32/37$; smem用了24KB，一個 SM 的 smem 大小是 96KB，以 smem 來說，最多一次可以同時執行 $96KB/24KB$ 個block; `threadsPerBlock = 32`，一個 SM 最多可以一次處理 2048 個 thread，以 thread 來說，最多一次可以處理 $2048/32$ 個 block，這樣算下來發現平行度會被smem bound 住，所以我將`UpdateMiLiOi` 其中一個shared memory 改成 global memory，讓 smem 用16KB就好，想犧牲一些access memory 的速度來去換取更大的平行度，結果發現是真的有提升將近一倍的 Performance。

### c. Others
從上面優化可以發現將 Br 調大會是很關鍵的一個點，所以也有嘗試將 Bc 調大，但發現並沒有太大的效果，我猜測應該是因為Bc和我的shared memory大小有關，所以 Bc 調大可能會導致上述優化第五點所說的 shared memory用太多，導致平行度下降，所以以 performance 沒有相差太多。
除此之外也有嘗試調整 `threadsPerBlock` 大小，不過performance也沒有變好，原先設定的`32` 就是最好的版本。
另外也有嘗試優化IO，不過因為這次寫入檔案的大小沒有很大，所以效果也不太明顯。

## 4. Experience & conclusion
一開始因為之前作業的經驗會很執著要調整shared memory的大小和mapping，但後來才發現原來調整B_r，讓kernel launch的次數大幅下降，在資料量很大筆的時候，也會對程式效能有很大的提升，這是之前比較沒有考慮到的問題。